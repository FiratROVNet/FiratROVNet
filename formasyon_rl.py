import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from FiratROVNet import senaryo


class BasitRL(nn.Module):
    """
    Basit RL SÄ±nÄ±fÄ±
    - 20 giriÅŸ
    - 1 gizli katman (64 nÃ¶ron)
    - 10 Ã§Ä±kÄ±ÅŸ
    """
    def __init__(self, learning_rate=0.001):
        super(BasitRL, self).__init__()
        
        # Neural Network KatmanlarÄ±
        self.giris_katman = nn.Linear(20, 64)
        self.gizli_katman = nn.ReLU()
        self.cikis_katman = nn.Linear(64, 10)
        self.softmax = nn.Softmax(dim=-1)
        
        # Optimizer
        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)
        
        # HafÄ±za (deneyimler iÃ§in)
        self.hafiza = []
        
    def forward(self, x):
        """
        Ä°leri geÃ§iÅŸ (forward pass)
        x: [batch_size, 20] giriÅŸ tensÃ¶rÃ¼
        Returns: [batch_size, 10] Ã§Ä±kÄ±ÅŸ olasÄ±lÄ±klarÄ±
        """
        x = self.giris_katman(x)
        x = self.gizli_katman(x)
        x = self.cikis_katman(x)
        return self.softmax(x)
    
    def aksiyon_sec(self, state):
        """
        Duruma gÃ¶re aksiyon seÃ§
        state: [20] numpy array veya tensor
        Returns: seÃ§ilen aksiyon indeksi (0-9)
        """
        if isinstance(state, np.ndarray):
            state = torch.FloatTensor(state).unsqueeze(0)
        
        with torch.no_grad():
            olasiliklar = self.forward(state)
            # OlasÄ±lÄ±klara gÃ¶re aksiyon seÃ§
            aksiyon = torch.multinomial(olasiliklar, 1).item()
        
        return aksiyon
    
    def hafizaya_ekle(self, state, action, reward, next_state=None, done=False):
        """
        Deneyimi hafÄ±zaya ekle
        """
        self.hafiza.append({
            'state': state,
            'action': action,
            'reward': reward,
            'next_state': next_state,
            'done': done
        })
    
    def ogren(self, gamma=0.99):
        """
        REINFORCE algoritmasÄ± ile Ã¶ÄŸrenme
        gamma: indirim faktÃ¶rÃ¼ (discount factor)
        """
        if len(self.hafiza) == 0:
            return
        
        # HafÄ±zadaki tÃ¼m deneyimleri iÅŸle
        states = []
        actions = []
        rewards = []
        
        for deneyim in self.hafiza:
            states.append(deneyim['state'])
            actions.append(deneyim['action'])
            rewards.append(deneyim['reward'])
        
        # Tensor'lara dÃ¶nÃ¼ÅŸtÃ¼r
        states = torch.FloatTensor(np.array(states))
        actions = torch.LongTensor(actions)
        rewards = np.array(rewards)
        
        # GecikmiÅŸ Ã¶dÃ¼lleri hesapla (discounted rewards)
        gecikmis_oduller = []
        G = 0
        for r in reversed(rewards):
            G = r + gamma * G
            gecikmis_oduller.insert(0, G)
        
        gecikmis_oduller = torch.FloatTensor(gecikmis_oduller)
        # Normalize et (opsiyonel, daha stabil eÄŸitim iÃ§in)
        gecikmis_oduller = (gecikmis_oduller - gecikmis_oduller.mean()) / (gecikmis_oduller.std() + 1e-8)
        
        # Policy gradient hesapla
        olasiliklar = self.forward(states)
        secilen_olasiliklar = olasiliklar.gather(1, actions.unsqueeze(1)).squeeze(1)
        
        # Loss: -log(pi(a|s)) * G (gradient ascent iÃ§in negatif)
        loss = -(torch.log(secilen_olasiliklar + 1e-8) * gecikmis_oduller).mean()
        
        # Optimize et
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        # HafÄ±zayÄ± temizle
        self.hafiza = []
        
        return loss.item()
    
    def hafizayi_temizle(self):
        """HafÄ±zayÄ± temizle"""
        self.hafiza = []


# Ã–dÃ¼l fonksiyonu (basit Ã¶rnek - ihtiyacÄ±nÄ±za gÃ¶re Ã¶zelleÅŸtirebilirsiniz)
def odul_hesapla(batarya_degerleri, sonar_degerleri, gps_koordinatlari):
    """
    Basit Ã¶dÃ¼l fonksiyonu
    - Batarya yÃ¼ksekse Ã¶dÃ¼l artar
    - Sonar mesafesi yÃ¼ksekse (engel yoksa) Ã¶dÃ¼l artar
    - ROV'lar birbirine Ã§ok yakÄ±nsa ceza
    """
    odul = 0.0
    
    # Batarya Ã¶dÃ¼lÃ¼ (ortalama batarya seviyesi)
    ortalama_batarya = np.mean(batarya_degerleri)
    odul += ortalama_batarya * 0.3
    
    # Sonar Ã¶dÃ¼lÃ¼ (engel mesafesi yÃ¼ksekse iyi)
    # Sonar -1 ise engel yok demektir, bu durumda maksimum Ã¶dÃ¼l
    sonar_odul = 0.0
    for sonar in sonar_degerleri:
        if sonar == -1:
            sonar_odul += 1.0  # Engel yok, maksimum Ã¶dÃ¼l
        else:
            # Sonar deÄŸerini normalize et (0-1 arasÄ±, yÃ¼ksek deÄŸer = iyi)
            # Ã–rnek: 50 birim mesafe varsa, normalize edilmiÅŸ deÄŸer ~0.5 olabilir
            normalized_sonar = min(sonar / 50.0, 1.0)  # 50 birim = maksimum
            sonar_odul += normalized_sonar
    odul += (sonar_odul / len(sonar_degerleri)) * 0.3
    
    # ROV'lar arasÄ± mesafe kontrolÃ¼ (Ã§ok yakÄ±nsa ceza)
    if len(gps_koordinatlari) >= 2:
        min_mesafe = float('inf')
        for i in range(len(gps_koordinatlari)):
            for j in range(i+1, len(gps_koordinatlari)):
                pos1 = gps_koordinatlari[i]
                pos2 = gps_koordinatlari[j]
                mesafe = np.sqrt((pos1[0]-pos2[0])**2 + (pos1[1]-pos2[1])**2 + (pos1[2]-pos2[2])**2)
                min_mesafe = min(min_mesafe, mesafe)
        
        # Minimum mesafe 10 birimden azsa ceza
        if min_mesafe < 10:
            odul -= 0.2
    
    return odul


def veri_topla():
    """
    4 ROV'dan veri toplar ve 20 boyutlu state vektÃ¶rÃ¼ oluÅŸturur.
    
    Returns:
        state: [20] numpy array
            [batarya_0, gps_x_0, gps_y_0, gps_z_0, sonar_0,
             batarya_1, gps_x_1, gps_y_1, gps_z_1, sonar_1,
             batarya_2, gps_x_2, gps_y_2, gps_z_2, sonar_2,
             batarya_3, gps_x_3, gps_y_3, gps_z_3, sonar_3]
    """
    state = []
    batarya_degerleri = []
    sonar_degerleri = []
    gps_koordinatlari = []
    
    for rov_id in range(4):
        # Batarya (0-1 arasÄ±)
        batarya = senaryo.get(rov_id, "batarya")
        if batarya is None:
            batarya = 0.5  # VarsayÄ±lan deÄŸer
        batarya_degerleri.append(batarya)
        state.append(batarya)
        
        # GPS (x, y, z)
        gps = senaryo.get(rov_id, "gps")
        if gps is None:
            gps = np.array([0.0, 0.0, 0.0])
        else:
            gps = np.array(gps)
        gps_koordinatlari.append(gps)
        
        # GPS koordinatlarÄ±nÄ± normalize et (Ã¶rnek: -200 ile 200 arasÄ± -> 0-1 arasÄ±)
        # Havuz geniÅŸliÄŸi 200 olduÄŸu iÃ§in koordinatlar -200 ile 200 arasÄ±nda olabilir
        normalized_x = (gps[0] + 200) / 400.0  # -200 -> 0, 200 -> 1
        normalized_y = (gps[1] + 200) / 400.0
        normalized_z = (gps[2] + 200) / 400.0
        
        state.extend([normalized_x, normalized_y, normalized_z])
        
        # Sonar (0-1 arasÄ± normalize edilmiÅŸ deÄŸer)
        sonar = senaryo.get(rov_id, "sonar")
        if sonar is None or sonar == -1:
            # Engel yok, normalize edilmiÅŸ deÄŸer = 1.0 (maksimum gÃ¼venlik)
            normalized_sonar = 1.0
            sonar_degerleri.append(-1)  # Ham deÄŸer iÃ§in -1 sakla
        else:
            # Sonar deÄŸerini normalize et (0-50 birim arasÄ± -> 0-1 arasÄ±)
            # Mesafe ne kadar bÃ¼yÃ¼kse o kadar iyi (1.0'a yakÄ±n)
            normalized_sonar = min(sonar / 50.0, 1.0)
            sonar_degerleri.append(sonar)  # Ham deÄŸeri sakla
        
        state.append(normalized_sonar)
    
    state = np.array(state, dtype=np.float32)
    
    return state, batarya_degerleri, sonar_degerleri, gps_koordinatlari


# EÄŸitim fonksiyonu
def egitim_baslat(epochs=500, n_rovs=4, n_engels=10, learning_rate=0.001):
    """
    500 epoch eÄŸitim baÅŸlatÄ±r.
    
    Args:
        epochs: EÄŸitim epoch sayÄ±sÄ± (varsayÄ±lan: 500)
        n_rovs: ROV sayÄ±sÄ± (varsayÄ±lan: 4)
        n_engels: Engel sayÄ±sÄ± (varsayÄ±lan: 10)
        learning_rate: Ã–ÄŸrenme oranÄ± (varsayÄ±lan: 0.001)
    """
    # RL AjanÄ±nÄ± oluÅŸtur
    rl_ajan = BasitRL(learning_rate=learning_rate)
    
    # Ä°lk kurulum (yavaÅŸ - ortam oluÅŸturulur)
    print("ðŸš€ Senaryo oluÅŸturuluyor...")
    senaryo.uret(n_rovs=n_rovs, n_engels=n_engels)
    print("âœ… Senaryo hazÄ±r!")
    
    # 500 Epoch EÄŸitim
    print(f"\nðŸŽ¯ EÄŸitim BaÅŸlÄ±yor ({epochs} Epoch)...")
    print("=" * 60)
    
    for epoch in range(epochs):
        # Senaryo pozisyonlarÄ±nÄ± gÃ¼ncelle (hÄ±zlÄ± - sadece pozisyonlar deÄŸiÅŸir)
        senaryo.uret()  # AynÄ± sayÄ±lar, farklÄ± koordinatlar
        
        # Veri topla
        state, batarya_degerleri, sonar_degerleri, gps_koordinatlari = veri_topla()
        
        # Aksiyon seÃ§ (formasyon seÃ§imi: 0-9 arasÄ±)
        aksiyon = rl_ajan.aksiyon_sec(state)
        
        # Ã–dÃ¼l hesapla
        odul = odul_hesapla(batarya_degerleri, sonar_degerleri, gps_koordinatlari)
        
        # HafÄ±zaya ekle
        rl_ajan.hafizaya_ekle(state, aksiyon, odul)
        
        # Her 32 deneyimde bir Ã¶ÄŸren (veya epoch sonunda)
        if len(rl_ajan.hafiza) >= 32:
            loss = rl_ajan.ogren()
            if epoch % 50 == 0:  # Her 50 epoch'ta bir loss yazdÄ±r
                print(f"   ðŸ”¹ Epoch {epoch}/{epochs} | Loss: {loss:.4f} | Ã–dÃ¼l: {odul:.4f} | Aksiyon: {aksiyon}")
        
        # Epoch sonunda kalan deneyimleri de Ã¶ÄŸren
        if epoch == epochs - 1 and len(rl_ajan.hafiza) > 0:
            loss = rl_ajan.ogren()
            print(f"   ðŸ”¹ Epoch {epoch+1}/{epochs} | Loss: {loss:.4f} | Ã–dÃ¼l: {odul:.4f} | Aksiyon: {aksiyon}")
    
    print("\nâœ… EÄŸitim tamamlandÄ±!")
    print("=" * 60)
    
    return rl_ajan


# KullanÄ±m Ã¶rneÄŸi
if __name__ == "__main__":
    # EÄŸitimi baÅŸlat
    rl_ajan = egitim_baslat(epochs=500, n_rovs=4, n_engels=10, learning_rate=0.001)
    
    # Modeli kaydet (opsiyonel)
    # torch.save(rl_ajan.state_dict(), 'formasyon_rl_model.pth')
    # print("ðŸ’¾ Model kaydedildi: formasyon_rl_model.pth")

